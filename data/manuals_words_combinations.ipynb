{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stop_list.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d4afc715527b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mexclusions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stop_list.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mexclusions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stop_list.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import jieba.posseg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "exclusions=[]\n",
    "with open(\"stop_list.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        exclusions.append(line.strip())\n",
    "def fetch_dic(stype):\n",
    "    with open('dics_dec.json') as f:\n",
    "        dics= json.load(f)\n",
    "    com=[]\n",
    "    for key,value in dics.items():\n",
    "        if value==stype:\n",
    "            com.append(key)\n",
    "    com.sort(key=len)\n",
    "    com=com[::-1]\n",
    "    return(com)\n",
    "\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 4, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dissuf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-819b03064449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mexl_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"u\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"p\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"pr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"d\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mExploration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocusedtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"og\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdissuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-819b03064449>\u001b[0m in \u001b[0;36mExploration\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mexl_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"u\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"p\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"pr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"d\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mExploration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocusedtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"og\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdissuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dissuf' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import copy\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "exl_tags=[\"u\",\"m\",\"x\",\"c\",\"p\",\"r\",\"pr\",\"d\"]  \n",
    "class Exploration():\n",
    "    def __init__(self, indications, focusedtype=\"og\",suffix=dissuf):\n",
    "        self.tags=[]\n",
    "        self.words=[]\n",
    "        self.sentences=[]\n",
    "        self.suffix=dissuf\n",
    "        self.kwposition=[]\n",
    "        self.entity1=[]\n",
    "        self.indexes=[]\n",
    "        self.ind_sent={}\n",
    "        for ix, j in enumerate(indications):\n",
    "            sent=j[\"sentence\"]\n",
    "            segs=j[\"seg\"]\n",
    "            dic_ind=j[\"ind\"]\n",
    "            try:\n",
    "                segstype=[s[1] for s in segs]\n",
    "                segsword=[s[0] for s in segs]\n",
    "            except:\n",
    "                print(print(j))\n",
    "                print(segs[0])\n",
    "                print(segs[0][1])\n",
    "                print()\n",
    "            for m,ss in enumerate(segstype):\n",
    "                if ss==focusedtype:\n",
    "                    mmax=np.min((m+5,len(segstype)))\n",
    "                    mmin=np.max((m-5,0))\n",
    "                    self.tags.append(segstype[mmin:mmax])\n",
    "                    self.words.append(segsword[mmin:mmax])\n",
    "                    self.kwposition.append(m-mmin)\n",
    "                    self.sentences.append(sent)\n",
    "                    self.entity1.append(j[\"entity1\"])\n",
    "                    self.indexes.append(dic_ind)\n",
    "        for (p,q) in zip(self.indexes,self.sentences):\n",
    "            self.ind_sent[p]=q\n",
    "        self.reorgnized_w,self.reorgnized_t,self.reorgnized_i=results=self.__processing__(focusedtype)\n",
    "#         self.prew=\n",
    "#         self.pret= \n",
    "    def __processing__(self,focusedtype):\n",
    "        all_t=[]\n",
    "        all_w=[]\n",
    "        quadruple_dic={}\n",
    "        for i,t in enumerate(self.tags):\n",
    "            try:\n",
    "                focal=t.index(focusedtype)\n",
    "            except:\n",
    "                print(t)\n",
    "            if focal==0:\n",
    "                continue\n",
    "            for n in range(-2,3,1):\n",
    "                if n==0:\n",
    "                    continue\n",
    "                # appending lists of words/tags fragments to the mega dict repectively\n",
    "                elif n<0 and focal+n!=0:\n",
    "                    start=np.max((0,focal+n))\n",
    "                    end=focal+1\n",
    "                    sliced=t[start:end]\n",
    "                elif n>0 and focal+n!=len(t):\n",
    "                    start=focal\n",
    "                    end=np.min((len(t),focal+n))+1\n",
    "                    sliced=t[start:end]\n",
    "                else:\n",
    "                    continue\n",
    "                conword=\" \".join(self.words[i][start:end])\n",
    "                w_key=\"w \"+str(n+2)+\"|\"+str(self.indexes[i])\n",
    "                t_key=\"t \"+str(n+2)+\"|\"+str(self.indexes[i])\n",
    "                quadruple_dic[w_key]=conword\n",
    "                quadruple_dic[t_key]=sliced   \n",
    "        self.quadruple_dic=quadruple_dic       \n",
    "        reorgnized_w=defaultdict(list)\n",
    "        reorgnized_t=defaultdict(list)\n",
    "        reorgnized_i=defaultdict(list)\n",
    "        for k,v in quadruple_dic.items():\n",
    "            order=int(k[2])\n",
    "            ind=int(k.split(\"|\")[1])\n",
    "            if k[0]==\"w\":\n",
    "                reorgnized_w[order].append(v) \n",
    "                reorgnized_i[order].append(ind)\n",
    "            else:\n",
    "                reorgnized_t[order].append(v) \n",
    "        ## 返回两个字典，分别是词 和 词性。不同key表示中心词前后不同位置。\n",
    "        return(reorgnized_w,reorgnized_t,reorgnized_i)\n",
    "\n",
    "    def plot(self, pos):\n",
    "        #展示特定位置的词的词性构成\n",
    "#         pie, ax = plt.subplots(figsize=[6,6])\n",
    "        s=pos+2\n",
    "        typefrequency=defaultdict(int)\n",
    "        \n",
    "    \n",
    "        if s-2<0:\n",
    "            subdict=[i[0] for i in self.reorgnized_t[s]]\n",
    "        else:\n",
    "            subdict=[i[-1] for i in self.reorgnized_t[s]]\n",
    "        for j in subdict:\n",
    "            typefrequency[j]+=1\n",
    "        labels = []\n",
    "        sizes = []\n",
    "        for x, y in typefrequency.items():\n",
    "            labels.append(x)\n",
    "            sizes.append(y)\n",
    "#         plt.pie(sizes, labels=labels, pctdistance=0.5)\n",
    "        sns.barplot(x=labels,y=sizes)\n",
    "#     def nested_plot(self, pos):\n",
    "        \n",
    "    def type_details(self,specifictype, pos):\n",
    "        # specifictype:想看的位置为pos的分词类型为specifictype的词\n",
    "        results=[]\n",
    "        if pos-2<0:\n",
    "            specific_t=[i[0] for i in self.reorgnized_t[pos+2]]\n",
    "        else:\n",
    "            specific_t=[i[-1] for i in self.reorgnized_t[pos+2]]\n",
    "        n_w=self.reorgnized_w[pos+2] \n",
    "        n_t=self.reorgnized_t[pos+2] \n",
    "        \n",
    "        for i,(t,w) in enumerate(zip(specific_t,n_w)):\n",
    "            if t==specifictype and not any(tag in n_t[i] for tag in exl_tags):\n",
    "                sentence_index=self.reorgnized_i[pos+2][i]\n",
    "                sentence=self.ind_sent[sentence_index]\n",
    "                new_entity=w.replace(\" \",\"\")\n",
    "                findind=sentence.find(new_entity)\n",
    "                display_sent=sentence[:findind]+\"【\"+new_entity+\"】\"+sentence[findind+len(new_entity):]\n",
    "                \n",
    "                result={}\n",
    "                result[\"words\"]=w\n",
    "                result[\"tags\"]=n_t[i]\n",
    "                result[\"sentence\"]=display_sent\n",
    "                results.append(result)\n",
    "        return(results)\n",
    "    \n",
    "    def is_a_in_x(self, A, X):\n",
    "        for i in range(len(X) - len(A) + 1):\n",
    "            if A == X[i:i+len(A)]: return (i,True)\n",
    "        return (0,False)\n",
    "    \n",
    "    def seq_examiner(self,seq):\n",
    "        for j in indications:\n",
    "            segs=[s[1] for s in j[\"seg\"]]\n",
    "            words=[s[0] for s in j[\"seg\"]]\n",
    "            (pos,contains)=self.is_a_in_x(seq,segs)\n",
    "            if contains: \n",
    "                print(words[pos-1:pos+4])\n",
    "                \n",
    "#     def seq_examiner_fuzzy(self,seq):\n",
    "#         for s in seq:\n",
    "#             if s==\"\":\n",
    "#                 for seg in \n",
    "#         for j in indications:\n",
    "#             segs=[s[1] for s in j[\"seg\"]]\n",
    "#             words=[s[0] for s in j[\"seg\"]]\n",
    "#             (pos,contains)=self.is_a_in_x(seq,segs)\n",
    "#             if contains: \n",
    "#                 print(words[pos-1:pos+4])\n",
    "    \n",
    "    def suffix_combiner(self,full_display=False):\n",
    "        for i,j in enumerate(self.words):\n",
    "            m=self.kwposition[i]\n",
    "            if len(j)<=1:\n",
    "                continue\n",
    "            suffix_included=[i for i in j[m:m+2] if i in self.suffix]\n",
    "            if suffix_included!=[]:\n",
    "                first_suffix=suffix_included[0]\n",
    "                until=j[m:].index(first_suffix)+m\n",
    "#                 print(j[m],self.tags[i][m],m,until)\n",
    "                if len(j[m:until+1])>1 and not any(tag in self.tags[i][m:until+1] for tag in exl_tags):\n",
    "                    sent=self.sentences[i]\n",
    "                    display_front=np.max((0,m-5))\n",
    "                    display_end=np.min((until+6,len(j)))\n",
    "                    new_entity=\"\".join(j[m:until+1])\n",
    "                    findind=sent.find(new_entity)\n",
    "                    \n",
    "                    display_sent=sent[:findind]+\"【\"+new_entity+\"】\"+sent[findind+len(new_entity):]\n",
    "                    wlist=self.sentences[i]\n",
    "                    ent1=self.entity1[i]\n",
    "#                     sent=\"\".join(wlist[:m])+\"【\"+\"\".join(wlist[m:until+1])+\"】\"+\"\".join(wlist[until+1:])\n",
    "                    #### 某系类型的分词词性 数字不能出现在短语中\n",
    "                    if m==0:\n",
    "                        continue \n",
    "                    if self.tags[i][m-1] in [\"og\",\"sy\",\"ds\",\"bc\",\"bf\"]:\n",
    "                        \n",
    "#                         start=j[\"seg\"][pos-1][2][1]\n",
    "#                         combined=\"\".join(words[pos:pos+len(seq)])\n",
    "#                         if \"、\" in combined:\n",
    "#                             continue\n",
    "#                         newly_merged[seqkey].append(combined)\n",
    "#                         j[\"seg\"][pos:pos+len(seq)]=[[combined,seq[-1],[start,start+len(combined)]]]\n",
    "                    \n",
    "                        print(ent1,j[m-1:until+1],self.tags[i][m-1:until+1],display_sent)\n",
    "                    else:\n",
    "                        print(ent1,j[m:until+1],self.tags[i][m:until+1],display_sent)\n",
    "                    if full_display==True:\n",
    "                        print(j,\"\\n\",self.tags[i])\n",
    "                    print()\n",
    "                    \n",
    "def in_between_inspector(rs,tag=\"v\"):\n",
    "    for dic in rs:\n",
    "        if tag in dic[\"tags\"]:\n",
    "            print(dic)\n",
    "def is_a_in_x(A, X):\n",
    "    for i in range(len(X) - len(A) + 1):\n",
    "        if A == X[i:i+len(A)]: return (i,True)\n",
    "    return (0,False)\n",
    "\n",
    "\n",
    "## 合并符合在 po_series 序列顺序的词为一个实体并存储，返回更新的全量数据\n",
    "def words_combiner(merged_results,po_series):\n",
    "# po_series=set(po_series)\n",
    "    indications_copy=copy.deepcopy(merged_results)\n",
    "    newly_merged=defaultdict(list)\n",
    "    for j in indications_copy:\n",
    "        j_dict={}\n",
    "        for z1,z2 in zip(list(range(0,len(j[\"seg\"]))),j[\"seg\"]):\n",
    "            j_dict[z1]=z2\n",
    "        \n",
    "        try:\n",
    "            segs=[s[1] for s in j[\"seg\"]]\n",
    "            words=[s[0] for s in j[\"seg\"]]\n",
    "        except:\n",
    "            print(j[\"seg\"])\n",
    "        for seq in po_series:\n",
    "            # iterating through all candidate sequences\n",
    "            seqkey=\"|\".join(seq)\n",
    "    #         newly_merged[seqkey].append(\"h\")\n",
    "            (pos,contains)=is_a_in_x(seq,segs)\n",
    "            if contains: \n",
    "                try:\n",
    "                    start=j[\"seg\"][pos-1][2][1]\n",
    "                    combined=\"\".join(words[pos:pos+len(seq)])\n",
    "                    if \"、\" in combined or combined not in j[\"sentence\"]:\n",
    "                        continue\n",
    "                    print(combined)\n",
    "                    newly_merged[seqkey].append(combined)\n",
    "#                     j[\"seg\"][pos:pos+len(seq)]=[[combined,seq[-1],[start,start+len(combined)]]]\n",
    "                    j_dict[pos]=[combined,seq[-1],[start,start+len(combined)]]\n",
    "                    del j_dict[pos+1]\n",
    "                    if len(seq)==3:\n",
    "                        del j_dict[pos+2]\n",
    "                except:\n",
    "                    print(j[\"seg\"])\n",
    "        j[\"seg\"]=list(j_dict.values())\n",
    "    lennew=0\n",
    "    for j in newly_merged.values():\n",
    "        lennew+=len(j)\n",
    "    print(\"newly discovered combinations:\",lennew) \n",
    "    return(indications_copy)\n",
    "#         newly_merged.append(new_words)\n",
    "\n",
    "## 合并后缀\n",
    "def suffix_combiner(merged_results,suffix,desiredlist=[\"og\",\"sy\",\"ds\",\"bc\",\"bf\"]):\n",
    "# po_series=set(po_series)\n",
    "    indications_copy=copy.deepcopy(merged_results)\n",
    "    newly_merged=defaultdict(list)\n",
    "    for j in indications_copy:\n",
    "        j_dict={}\n",
    "        \n",
    "        for z1,z2 in zip(list(range(0,len(j[\"seg\"]))),j[\"seg\"]):\n",
    "            j_dict[z1]=z2\n",
    "        segs=[s[1] for s in j[\"seg\"]]\n",
    "        words=[s[0] for s in j[\"seg\"]]\n",
    "        for dt,md in enumerate(j[\"seg\"][:-1]):\n",
    "            if md[1] in desiredlist and j[\"seg\"][dt+1][0] in suffix:\n",
    "                pos=dt\n",
    "                start=j[\"seg\"][pos-1][2][1]\n",
    "                combined=\"\".join(words[pos:pos+2])\n",
    "                if \"、\" in combined or combined not in j[\"sentence\"]:\n",
    "                    continue\n",
    "                newly_merged[seqkey].append(combined)\n",
    "                print(combined)\n",
    "                j_dict[pos]=[combined,seq[-1],[start,start+len(combined)]]\n",
    "                del j_dict[pos+1]\n",
    "#                     j[\"seg\"][pos:pos+len(seq)]=[[combined,seq[-1],[start,start+len(combined)]]]\n",
    "    #             j[\"seg\"[pos:pos+len(seq)]=[\"\".join(words[pos:pos+len(seq)]),seq[-1]]\n",
    "        j[\"seg\"]=list(j_dict.values())\n",
    "#     print(j[\"seg\"])\n",
    "    lennew=0\n",
    "    for j in newly_merged.values():\n",
    "        lennew+=len(j)\n",
    "    print(\"newly discovered combinations:\",lennew) \n",
    "    return(indications_copy)\n",
    "#         newly_merged.append(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
